{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# ðŸ§  Fine-tuning LoRA - David WhatsApp Clone\n",
    "\n",
    "# âœ… Ã‰tape 1 : Installer les dÃ©pendances\n",
    "!pip install -q peft bitsandbytes accelerate transformers datasets trl\n",
    "\n",
    "# âœ… Ã‰tape 2 : Importer les bibliothÃ¨ques\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training, TaskType\n",
    "from datasets import load_dataset, Dataset\n",
    "from trl import SFTTrainer\n",
    "import torch\n",
    "import json\n",
    "\n",
    "# âœ… Ã‰tape 3 : Uploader ton dataset (format HuggingFace style)\n",
    "from google.colab import files\n",
    "uploaded = files.upload()\n",
    "\n",
    "# âœ… Ã‰tape 4 : Charger le dataset local\n",
    "with open(\"david_conversations.json\", \"r\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "all_rows = []\n",
    "for convo in data:\n",
    "    for i in range(1, len(convo['conversations']), 2):\n",
    "        user_msg = convo['conversations'][i-1]['content']\n",
    "        david_msg = convo['conversations'][i]['content']\n",
    "        prompt = f\"<|user|>\\n{user_msg}\\n<|assistant|>\\n{david_msg}\"\n",
    "        all_rows.append({\"text\": prompt})\n",
    "\n",
    "dataset = Dataset.from_list(all_rows)\n",
    "\n",
    "# âœ… Ã‰tape 5 : Charger le modÃ¨le (Zephyr ou Mistral)\n",
    "model_id = \"mistralai/Mistral-7B-Instruct-v0.1\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, use_fast=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, load_in_4bit=True, torch_dtype=torch.float16, device_map=\"auto\")\n",
    "\n",
    "# âœ… Ã‰tape 6 : Appliquer LoRA\n",
    "lora_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    ")\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "# âœ… Ã‰tape 7 : Configurer l'entraÃ®nement\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"david-lora-output\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=2,\n",
    "    gradient_accumulation_steps=4,\n",
    "    logging_steps=10,\n",
    "    learning_rate=2e-4,\n",
    "    fp16=True,\n",
    "    save_strategy=\"epoch\",\n",
    "    report_to=\"none\",\n",
    ")\n",
    "\n",
    "# âœ… Ã‰tape 8 : Lancer le fine-tuning\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_args,\n",
    "    packing=True,\n",
    "    dataset_text_field=\"text\",\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "# âœ… Ã‰tape 9 : Sauvegarder dans Google Drive (si montÃ©)\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "# model.save_pretrained(\"/content/drive/MyDrive/david-lora-model\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
